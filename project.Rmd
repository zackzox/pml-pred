---
title: "PML-project: Prediction Assignment Submission"
author: "Zack Zox"
date: '`r format(Sys.Date(), "%d %B, %Y")`'
output:
  html_document:
    toc: yes
    number_sections: true
    css: custom-2.css
    fig_width: 7
    fig_height: 4
    fig_caption: true
---

---

```{r setup, echo=FALSE, warning = F,message=F}
library(knitr)
opts_chunk$set(echo = FALSE, comment=NA)
library(ggplot2)
library(caret)
library(doMC)
library(reshape2)
library(dplyr)
library(randomForest)
```

```{r help, warnings=F}
# plots missingness
ggplot_missing <- function(x){
  x %>% 
    is.na %>%
    melt %>%
    ggplot(data = ., aes(x = Var2, y = Var1)) +  geom_raster(aes(fill = value)) +
    scale_fill_grey(name = "",labels = c("Present","Missing")) +
    theme_minimal() +theme(axis.text.x=element_blank()) + # theme(axis.text.x  = element_text(angle=45, vjust=0.5,size=0)) + 
    labs(x = "Variables", y = "Observations")
}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("assgn/problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
```

# Background  

Devices such as Jawbone Up, Nike FuelBand, and Fitbit are used to collect data about personal activity. Six participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in 5 different ways. The objective is to find how well the movements were done. The five ways (A:right and B-C-D-E:wrong) are:  

* A- exactly according to the specification 
* B- throwing the elbows to the front
* C- lifting the dumbbell only halfway 
* D- lowering the dumbbell only halfway
* E- throwing the hips to the front

# Data 
## Description 

Data from accelerometers on the belt, forearm, arm, and dumbell of the participants were sampled (at frequency of 45Hz) during the whole sequence, with identification of the class of movement for each sampling. Prediction of exercise class must be done only with data from one sampling (no complete movement present in the test data set).

* **Training data** set has 19 622 observations with 159 variables. This is an average of 65 samplings during each of the 300 movements (6 participants x 5 types x 10 repetitions). Following is the breakdown of the samplings by classes. 
```{r data}
training <- read.csv("pml-training.csv", row.names = 1, na.strings=c("NA","","#DIV/0!"))
testing  <- read.csv("pml-testing.csv", row.names = 1,  na.strings=c("NA","","#DIV/0!"))
table(training$classe)
```

* **Testing data** set has 20 samplings on the same 159 variables. 

## Features identification  
* The first 6 columns are used for sampling identification and are removed from the features list. 
* Many features have missing values both in the testing set and in the training set.  Following figure shows distribution of missing columns for testing set. Same situation for the training set. Features with missing values are removed. (A feature was removed if more than 90% of its values are missing.) A test was done on the remaining features for near-zero-variation values.  None was found.  
* All features were centered and scaled on the training set. Same operations with same parameters were applied on the test set.

```{r identification}
    # Drop first columns
training <- training[,-c(1:6)]
testing  <-  testing[,-c(1:6)]
    # visualize missing values on testing set
ggplot_missing(testing)
    # remove if missing more than 90%
tresh <- nrow(training) * 0.90      
missColumns <- apply(training, 2, function(x) sum(is.na(x)) > tresh )
    # sum(missColumns)
training <- training[,!missColumns]
testing <-   testing[,!missColumns]
    # test for near zero features
nzv <- nearZeroVar(training, saveMetrics = T)
    # sum(nzv$nzv)         # none
    # test correlation
# predictors <- names(training)[names(training) != "classe"]
# predCor= cor(training[,predictors])
# highCor=findCorrelation(predCor,0.90)
#     # names(training[,highCor])
# training <- training[,-highCor]
# testing  <-  testing[,-highCor]
    # ncol(training)
    #pre-processing
procValues <- preProcess(training, method = c("center", "scale"))
training <- predict(procValues, training)
testing  <- predict(procValues, testing)
    # features 
predictors <- names(training)[names(training) != "classe"]
```

* We have `r length(predictors)` features available for modelling. 

# Model building 

```{r random forest, cache=TRUE}
#Partition rows into training and validation  
set.seed(123876)
inTrain = createDataPartition(training$classe, p = 0.60, list=FALSE)
train    = training[ inTrain,]
valid    = training[-inTrain,]

registerDoMC(cores = 3)
ctrl=  trainControl(method = "cv",repeats= 3)    
grid <- expand.grid( mtry = c(3,7,10,14))
rf_model= train( y= train$classe, x=train[,predictors] ,
                 method="rf", trControl=ctrl, tuneGrid = grid, ntree=100) 
```

We supposed that there would be many intersections between samplings of different classes of movement and classification for these cases would be difficult. We planned to use a non-linear model that allows interactions. We selected « random forest » as the method of prediction. It was successful, so we did not test any other model. Since the classes are well balanced,  we used *accuracy* as a measure of model performance.

In random forests, there is less need for cross-validation or a separate test set to get an unbiased estimate of the test set error, since it is estimated internally, during the run. The unbiased generalization error is calculated on out-of-bag (OOB) samples (about one third of the samples) not used in the bootstrap sample from the original data for each tree. (from documentation) 

But to verify this assertion, we decided to split the samplings into two sets: *training* (60%) and *validation* (40%). Prediction on the validation set will be calculated.

## Parameters tuning
* Random forest in *caret* has only one parameter: *mtry*, the number of variables randomly sampled as candidates at each split. Typically, for a classification problem with p features, √p features are used in each split (7 for our problem). We chose to test *mtry* for 3,7,10,14 features at each split. 
* We limited the number of trees generated (default = 500) to 100. 
* Parameter tuning was done through 3 cross-validation runs (10 folds)

The model accuracy is quite high for all values. There is no noticeable difference between the values of mtry, accuracy going from 99,0% to 99,2%.  The best accuracy of 99,2 % is with mtry=10 features.

```{r results}
print(rf_model)
#plot(rf_model,xlab='Value of parameter= mtry')
``` 

## Model validation  
```{r v1}
er=rf_model$finalModel$err.rate
y=round(er[dim(er)[1],1],3)*100
```
Accuracy converges rapidly around 60 trees. So there is no need to increase the number of trees.

### OOB error
Final model has an Out-Of-Bag error rate of `r y`% on the training set. This OOB error is a good estimate of the generalization error.

```{r validation}
er=plot(rf_model$finalModel,main='Model performance')
for(i in 1:dim(er)[2]) 
  text( dim(er)[1],er[dim(er)[1],i], (attr(er,"dimnames")[[2]])[i], cex =0.60)
text(dim(er)[1]-5,er[1,1], paste((attr(er,"dimnames")[[2]])[1] ,round(er[dim(er)[1],1],3)), cex = 0.75)

# for(i in 1:dim(er)[2]) 
#   print(paste( (attr(er,"dimnames")[[2]])[i] , round(er[1,i],3) , round(er[dim(er)[1],i],3) ))

# rfout=rf_model$finalModel
# mean( predict( rfout ) != train$class ) #OOB error rate on the training data.
# mean( predict( rfout, newdata = train ) != train$classe ) # Zero!
# mean( predict( rfout, newdata = valid ) != valid$classe) # Error rate on the test data.
```

```{r valid}
cvPred <- predict(rf_model, valid)
z=confusionMatrix(cvPred, valid$classe)
y=(1-round(z$overall[1],3))*100
# rf_model$times
# save(rf_model, file="rf_model-imp.RData")
# load(file="rf_model.RData")
```  
### Validation set error  
How accurately can we generalize on the validation set?  Using the final model, we predicted the classes on the validation set to get an independent assessment of model accuracy.
We have a `r y`% error rate (`r 100-y`% accuracy) for the validation set. So we are confident with the model built.

```{r valid2}
z 
```  

# Prediction on the test set 

Finally, the prediction model was run on the test data to predict the classes for the 20 test cases and the submission files were created.

```{r test-set, echo=TRUE}
(testingPred <- predict(rf_model, testing ))
```
```{r pml}
pml_write_files(as.character(testingPred))
# ref=c("B","A","B","A","A","E","D","B","A","A","B","C","B","A","E","E","A","B","B","B")
# identical(ref, as.character(testingPred))
```
